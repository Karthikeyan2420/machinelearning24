{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffcd608-4423-461c-a543-c4a2a677f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['text', 'mining', 'is', 'the', 'process', 'of', 'extracting', 'meaningful', 'information', 'from', 'text', 'data', 'it', 'involves', 'transforming', 'unstructured', 'data', 'into', 'structured', 'data', 'for', 'analysis']\n",
      "Filtered Tokens (No Stop Words): ['text', 'mining', 'process', 'extracting', 'meaningful', 'information', 'text', 'data', 'involves', 'transforming', 'unstructured', 'data', 'structured', 'data', 'analysis']\n",
      "Word Frequencies: Counter({'data': 3, 'text': 2, 'mining': 1, 'process': 1, 'extracting': 1, 'meaningful': 1, 'information': 1, 'involves': 1, 'transforming': 1, 'unstructured': 1, 'structured': 1, 'analysis': 1})\n",
      "Most Common Words: [('data', 3), ('text', 2), ('mining', 1), ('process', 1), ('extracting', 1)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"Text mining is the process of extracting meaningful information from text data. \n",
    "It involves transforming unstructured data into structured data for analysis.\"\"\"\n",
    "\n",
    "# Step 1: Preprocess the text (convert to lowercase and remove punctuation)\n",
    "text_cleaned = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "# Step 2: Tokenize the text (split into words)\n",
    "tokens = text_cleaned.split()\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 3: Remove stop words manually\n",
    "stop_words = {\"is\", \"the\", \"of\", \"from\", \"it\", \"for\", \"and\", \"into\"}\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Filtered Tokens (No Stop Words):\", filtered_tokens)\n",
    "\n",
    "# Step 4: Count word frequencies\n",
    "word_counts = Counter(filtered_tokens)\n",
    "print(\"Word Frequencies:\", word_counts)\n",
    "\n",
    "# Step 5: Identify the most common words\n",
    "most_common_words = word_counts.most_common(5)\n",
    "print(\"Most Common Words:\", most_common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004c41e9-c154-49a2-8cb1-c933b5be4566",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required datasets (first-time use only)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"Natural Language Processing (NLP) is a sub-field of artificial intelligence that deals \n",
    "with the interaction between computers and humans using natural language.\"\"\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokenized Words:\", tokens)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Words (No Stop Words):\", filtered_words)\n",
    "\n",
    "# Perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fba85f-be75-4276-95d5-630108e06583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
